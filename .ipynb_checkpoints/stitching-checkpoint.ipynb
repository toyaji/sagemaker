{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (4.2.0.34)\n",
      "Requirement already satisfied: opencv-contrib-python in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (4.4.0.46)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from opencv-python) (1.18.1)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.3.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install opencv-python opencv-contrib-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stitch(input_images, gpu=False, blend_strength=5):\n",
    "    \n",
    "    # I tried SIFT, SURF, but it is prohibited to use for patent. So, I go with ORB for feature extration\n",
    "    feature_finder = cv2.ORB.create()\n",
    "    \n",
    "    full_img_sizes = []\n",
    "    features = []\n",
    "    images = []\n",
    "    \n",
    "    # find feature alogn with images\n",
    "    for img in input_images:\n",
    "            \n",
    "        full_img_sizes.append((img.shape[1], img.shape[0]))\n",
    "        img_features = cv2.detail.computeImageFeatures2(feature_finder, img)\n",
    "        features.append(img_features)\n",
    "        images.append(img)\n",
    "    \n",
    "    # To set the configuration threshhold, I followed the stitching_detailed.py stitchinig pipelins' default recommendation.\n",
    "    matcher = cv2.detail.BestOf2NearestMatcher_create(gpu, 0.3)\n",
    "    p = matcher.apply2(features)\n",
    "    matcher.collectGarbage() # For memory reuse\n",
    "    \n",
    "    #from here, I used homography estimator\n",
    "    estimator = cv2.detail_HomographyBasedEstimator()\n",
    "    ret, cameras = estimator.apply(features, p, None)\n",
    "    if not ret: \n",
    "        print(\"Homography estimation failed\")\n",
    "        exit()\n",
    "    for cam in cameras: cam.R = cam.R.astype(np.float32)\n",
    "    \n",
    "    \n",
    "    # from here, adjuster, I followed lowe's methos using rporjection error squares.\n",
    "    adjuster = cv2.detail_BundleAdjusterReproj()\n",
    "    adjuster.setConfThresh(1)\n",
    "    ret, cameras = adjuster.apply(features, p, cameras)\n",
    "    if not ret:\n",
    "        print(\"Camera parameters adjusting failed\")\n",
    "        exit()\n",
    "    \n",
    "    \n",
    "    # frome here, to correct warp and wave effect on horizon line: and my case is only two camera that is different from sample code\n",
    "    # This part is needed to be retrimed\n",
    "    focals = []\n",
    "    for cam in cameras:\n",
    "        focals.append(cam.focal)\n",
    "    focals.sort()\n",
    "    \n",
    "    warped_image_scale = (focals[1] + focals[0]) / 2\n",
    "    rmats = cv2.detail.waveCorrect([cameras[0].R, cameras[1].R], cv2.detail.WAVE_CORRECT_HORIZ)\n",
    "    \n",
    "    for idx, cam in enumerate(cameras):\n",
    "        cam.R = rmats[idx]\n",
    "        \n",
    "    corners = []\n",
    "    masks_warped = []\n",
    "    images_warped = []\n",
    "    sizes = []\n",
    "    masks = []\n",
    "    \n",
    "    # it should be more than 2 if you stitch more than two images. but my case is using only two images for each stitching\n",
    "    for i in range(0, 2):\n",
    "        # This umat locastes the matrix into memory rather than using CPU\n",
    "        um =  cv2.UMat(255 * np.ones((images[i].shape[0], images[i].shape[1]), np.uint8))\n",
    "        masks.append(um)\n",
    "        \n",
    "    # this warper need to set a type of warping, \n",
    "    # but innner of this function is just consist of many types of warper like cynliderical, stero, etcc\n",
    "    # And for I choose work_scale and seam_sacle as 0, just considered warped scale\n",
    "    # From here the composting part, so I need to reorganize from here to apply for video stitching using same homography to all frames\n",
    "    warper = cv2.PyRotationWarper('cylindrical', warped_image_scale)\n",
    "    for idx in range(0,2):\n",
    "        K = cameras[idx].K().astype(np.float32)\n",
    "        corner, image_wp = warper.warp(images[idx], K, cameras[idx].R, cv2.INTER_LINEAR, cv2.BORDER_REFLECT)\n",
    "        corners.append(corner)\n",
    "        # I doubh this warped images having different size every time? \n",
    "        sizes.append((image_wp.shape[1], image_wp.shape[0]))\n",
    "        images_warped.append(image_wp)\n",
    "        \n",
    "        p, mask_wp = warper.warp(masks[idx], K, cameras[idx].R, cv2.INTER_LINEAR, cv2.BORDER_CONSTANT)\n",
    "        masks_warped.append(mask_wp.get())\n",
    "        \n",
    "    \n",
    "    images_warped_f = []\n",
    "    for img in images_warped:\n",
    "        imgf = img.astype(np.float32)\n",
    "        images_warped_f.append(imgf)\n",
    "    \n",
    "    \n",
    "    # from here, compensation step / also I followed lowe's metho using 'gain compensation'\n",
    "    compensator = cv2.detail.ExposureCompensator_createDefault(cv2.detail.ExposureCompensator_GAIN)\n",
    "    compensator.feed(corners, images_warped, masks_warped)\n",
    "    \n",
    "    # It might occur error for the type of image like nu.float32\n",
    "    seam_finder = cv2.detail_GraphCutSeamFinder('COST_COLOR_GRAD')\n",
    "    seam_finder.find(images_warped_f, corners, masks_warped)\n",
    "    \n",
    "    compose_sccale = 1\n",
    "    \n",
    "    \n",
    "    for idx, img in enumerate(images):\n",
    "        \n",
    "        dilated_mask = cv2.dilate(masks_warped[idx], None)\n",
    "        seam_mask = cv2.resize(dilated_mask, (masks_warped[idx].shape[1], masks_warped[idx].shape[0]), 0, 0, cv2.INTER_LINEAR_EXACT)\n",
    "        mask_warped = cv2.bitwise_and(dilated_mask, seam_mask)\n",
    "        \n",
    "        \"\"\"\n",
    "        # I used the multiblender as like lowe's paper\n",
    "        dst_sz = cv2.detail.resultRoi(corners, sizes)\n",
    "        blend_width = np.sqrt(dst_sz[2] * dst_sz[3]) * blend_strength / 100\n",
    "        blender = cv2.detail_MultiBandBlender()\n",
    "        blender.setNumBands((np.log(blend_width) / np.log(2.) - 1).astype(np.int))\n",
    "        blender.prepare(dst_sz)\n",
    "        \"\"\"\n",
    "        \n",
    "        # following code for fixed multiban for timelapse video\n",
    "        timelapser = cv2.detail.Timelapser_createDefault(cv2.detail.Timelapser_AS_IS)\n",
    "        timelapser.initialize(corners, sizes)\n",
    "        image_warped_s = images_warped[idx].astype(np.int16)\n",
    "        ma_tones = np.ones((image_warped_s.shape[0], image_warped_s.shape[1]), np.uint8)\n",
    "        timelapser.process(image_warped_s, ma_tones, corners[idx])\n",
    "        dst = timelapser.getDst()\n",
    "        \n",
    "    return cameras, images_warped, dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "right_video = cv2.VideoCapture(\"./data/right.mp4\")\n",
    "left_video = cv2.VideoCapture(\"./data/left.mp4\")\n",
    "ret, right_img = right_video.read()\n",
    "_, left_img = left_video.read()\n",
    "images = [left_img, right_img]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cameras, images_warped, dst = stitch(images, gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1257.280374825999"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cameras[0].focal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "warped_image_scale = (cameras[1].focal + cameras[0].focal) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "seam_scale = min(1.0, np.sqrt(0.1 * 1e6 / (images[0].shape[0] * images[0].shape[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "213.00076681926427"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "warped_image_scale * seam_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = cameras[0].K().astype(np.float32)\n",
    "warper = cv2.PyRotationWarper('plane', 300)\n",
    "corner, image_wp = warper.warp(images[0], K, cameras[0].R, cv2.INTER_LINEAR, cv2.BORDER_REFLECT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_imshow(img):\n",
    "    plt.figure(figsize=(8,12))\n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "right_video = cv2.VideoCapture(\"./data/right.mp4\")\n",
    "left_video = cv2.VideoCapture(\"./data/left.mp4\")\n",
    "ret, right_img = right_video.read()\n",
    "_, left_img = left_video.read()\n",
    "\n",
    "cv2.imwrite('./data/right_img.jpg', right_img)\n",
    "cv2.imwrite('./data/left_img.jpg', left_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_finder = cv2.ORB.create()\n",
    "    \n",
    "full_img_sizes = []\n",
    "features = []\n",
    "images = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_images = [left_img, right_img]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2560"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "left_img.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find feature alogn with images\n",
    "for img in input_images:\n",
    "    full_img_sizes.append((img.shape[1], img.shape[0]))\n",
    "    img_features = cv2.detail.computeImageFeatures2(feature_finder, img)\n",
    "    features.append(img_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = cv2.detail.BestOf2NearestMatcher_create(True, 0.3)\n",
    "p = matcher.apply2(features)\n",
    "matcher.collectGarbage() # For memory reuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = cv2.detail_HomographyBasedEstimator()\n",
    "ret, cameras = estimator.apply(features, p, None)\n",
    "if not ret: \n",
    "    print(\"Homography estimation failed\")\n",
    "    exit()\n",
    "for cam in cameras: cam.R = cam.R.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([<DMatch 0x7f5c2fdc9fb0>,\n",
       "  <DMatch 0x7f5c2f904a50>,\n",
       "  <DMatch 0x7f5c2f904b30>,\n",
       "  <DMatch 0x7f5c2f904a10>,\n",
       "  <DMatch 0x7f5c2f904b90>,\n",
       "  <DMatch 0x7f5c2f904b70>,\n",
       "  <DMatch 0x7f5c2f904b50>,\n",
       "  <DMatch 0x7f5c2f904ab0>,\n",
       "  <DMatch 0x7f5c2f904ad0>,\n",
       "  <DMatch 0x7f5c2f904a90>,\n",
       "  <DMatch 0x7f5c2f904a30>,\n",
       "  <DMatch 0x7f5c2f904bb0>,\n",
       "  <DMatch 0x7f5c2f904bd0>,\n",
       "  <DMatch 0x7f5c2f904bf0>,\n",
       "  <DMatch 0x7f5c2f904c10>,\n",
       "  <DMatch 0x7f5c2f904c30>,\n",
       "  <DMatch 0x7f5c2f904c50>,\n",
       "  <DMatch 0x7f5c2f904c70>,\n",
       "  <DMatch 0x7f5c2f904c90>,\n",
       "  <DMatch 0x7f5c2f904cb0>,\n",
       "  <DMatch 0x7f5c2f904cd0>,\n",
       "  <DMatch 0x7f5c2f904cf0>,\n",
       "  <DMatch 0x7f5c2f904d10>,\n",
       "  <DMatch 0x7f5c2f904d30>,\n",
       "  <DMatch 0x7f5c2f904d50>,\n",
       "  <DMatch 0x7f5c2f904d70>,\n",
       "  <DMatch 0x7f5c2f904d90>,\n",
       "  <DMatch 0x7f5c2f904db0>,\n",
       "  <DMatch 0x7f5c2f904dd0>,\n",
       "  <DMatch 0x7f5c2f904df0>,\n",
       "  <DMatch 0x7f5c2f904e10>,\n",
       "  <DMatch 0x7f5c2f904e30>,\n",
       "  <DMatch 0x7f5c2f904e50>,\n",
       "  <DMatch 0x7f5c2f904e70>,\n",
       "  <DMatch 0x7f5c2f904e90>,\n",
       "  <DMatch 0x7f5c2f904eb0>,\n",
       "  <DMatch 0x7f5c2f904ed0>,\n",
       "  <DMatch 0x7f5c2f904ef0>,\n",
       "  <DMatch 0x7f5c2f904f10>,\n",
       "  <DMatch 0x7f5c2f904f30>,\n",
       "  <DMatch 0x7f5c2f904f50>,\n",
       "  <DMatch 0x7f5c2f904f70>,\n",
       "  <DMatch 0x7f5c2f904f90>,\n",
       "  <DMatch 0x7f5c2f904fb0>,\n",
       "  <DMatch 0x7f5c2f904fd0>,\n",
       "  <DMatch 0x7f5c2f933030>,\n",
       "  <DMatch 0x7f5c2f933050>,\n",
       "  <DMatch 0x7f5c2f933070>,\n",
       "  <DMatch 0x7f5c2f933090>,\n",
       "  <DMatch 0x7f5c2f9330b0>,\n",
       "  <DMatch 0x7f5c2f9330d0>,\n",
       "  <DMatch 0x7f5c2f9330f0>,\n",
       "  <DMatch 0x7f5c2f933110>,\n",
       "  <DMatch 0x7f5c2f933130>,\n",
       "  <DMatch 0x7f5c2f933150>,\n",
       "  <DMatch 0x7f5c2f933170>,\n",
       "  <DMatch 0x7f5c2f933190>,\n",
       "  <DMatch 0x7f5c2f9331b0>,\n",
       "  <DMatch 0x7f5c2f9331d0>,\n",
       "  <DMatch 0x7f5c2f9331f0>,\n",
       "  <DMatch 0x7f5c2f933210>,\n",
       "  <DMatch 0x7f5c2f933230>,\n",
       "  <DMatch 0x7f5c2f933250>,\n",
       "  <DMatch 0x7f5c2f933270>,\n",
       "  <DMatch 0x7f5c2f933290>,\n",
       "  <DMatch 0x7f5c2f9332b0>,\n",
       "  <DMatch 0x7f5c2f9332d0>,\n",
       "  <DMatch 0x7f5c2f9332f0>,\n",
       "  <DMatch 0x7f5c2f933310>,\n",
       "  <DMatch 0x7f5c2f933330>,\n",
       "  <DMatch 0x7f5c2f933350>,\n",
       "  <DMatch 0x7f5c2f933370>,\n",
       "  <DMatch 0x7f5c2f933390>,\n",
       "  <DMatch 0x7f5c2f9333b0>,\n",
       "  <DMatch 0x7f5c2f9333d0>,\n",
       "  <DMatch 0x7f5c2f9333f0>,\n",
       "  <DMatch 0x7f5c2f933410>,\n",
       "  <DMatch 0x7f5c2f933430>,\n",
       "  <DMatch 0x7f5c2f933450>,\n",
       "  <DMatch 0x7f5c2f933470>,\n",
       "  <DMatch 0x7f5c2f933490>,\n",
       "  <DMatch 0x7f5c2f9334b0>],\n",
       " [<DMatch 0x7f5c2f9334d0>,\n",
       "  <DMatch 0x7f5c2f9334f0>,\n",
       "  <DMatch 0x7f5c2f933510>,\n",
       "  <DMatch 0x7f5c2f933530>,\n",
       "  <DMatch 0x7f5c2f933550>,\n",
       "  <DMatch 0x7f5c2f933570>,\n",
       "  <DMatch 0x7f5c2f933590>,\n",
       "  <DMatch 0x7f5c2f9335b0>,\n",
       "  <DMatch 0x7f5c2f9335d0>,\n",
       "  <DMatch 0x7f5c2f9335f0>,\n",
       "  <DMatch 0x7f5c2f933610>,\n",
       "  <DMatch 0x7f5c2f933630>,\n",
       "  <DMatch 0x7f5c2f933650>,\n",
       "  <DMatch 0x7f5c2f933670>,\n",
       "  <DMatch 0x7f5c2f933690>,\n",
       "  <DMatch 0x7f5c2f9336b0>,\n",
       "  <DMatch 0x7f5c2f9336d0>,\n",
       "  <DMatch 0x7f5c2f9336f0>,\n",
       "  <DMatch 0x7f5c2f933710>,\n",
       "  <DMatch 0x7f5c2f933730>,\n",
       "  <DMatch 0x7f5c2f933750>,\n",
       "  <DMatch 0x7f5c2f933770>,\n",
       "  <DMatch 0x7f5c2f933790>,\n",
       "  <DMatch 0x7f5c2f9337b0>,\n",
       "  <DMatch 0x7f5c2f9337d0>,\n",
       "  <DMatch 0x7f5c2f9337f0>,\n",
       "  <DMatch 0x7f5c2f933810>,\n",
       "  <DMatch 0x7f5c2f933830>,\n",
       "  <DMatch 0x7f5c2f933850>,\n",
       "  <DMatch 0x7f5c2f933870>,\n",
       "  <DMatch 0x7f5c2f933890>,\n",
       "  <DMatch 0x7f5c2f9338b0>,\n",
       "  <DMatch 0x7f5c2f9338d0>,\n",
       "  <DMatch 0x7f5c2f9338f0>,\n",
       "  <DMatch 0x7f5c2f933910>,\n",
       "  <DMatch 0x7f5c2f933930>,\n",
       "  <DMatch 0x7f5c2f933950>,\n",
       "  <DMatch 0x7f5c2f933970>,\n",
       "  <DMatch 0x7f5c2f933990>,\n",
       "  <DMatch 0x7f5c2f9339b0>,\n",
       "  <DMatch 0x7f5c2f9339d0>,\n",
       "  <DMatch 0x7f5c2f9339f0>,\n",
       "  <DMatch 0x7f5c2f933a10>,\n",
       "  <DMatch 0x7f5c2f933a30>,\n",
       "  <DMatch 0x7f5c2f933a50>,\n",
       "  <DMatch 0x7f5c2f933a70>,\n",
       "  <DMatch 0x7f5c2f933a90>,\n",
       "  <DMatch 0x7f5c2f933ab0>,\n",
       "  <DMatch 0x7f5c2f933ad0>,\n",
       "  <DMatch 0x7f5c2f933af0>,\n",
       "  <DMatch 0x7f5c2f933b10>,\n",
       "  <DMatch 0x7f5c2f933b30>,\n",
       "  <DMatch 0x7f5c2f933b50>,\n",
       "  <DMatch 0x7f5c2f933b70>,\n",
       "  <DMatch 0x7f5c2f933b90>,\n",
       "  <DMatch 0x7f5c2f933bb0>,\n",
       "  <DMatch 0x7f5c2f933bd0>,\n",
       "  <DMatch 0x7f5c2f933bf0>,\n",
       "  <DMatch 0x7f5c2f933c10>,\n",
       "  <DMatch 0x7f5c2f933c30>,\n",
       "  <DMatch 0x7f5c2f933c50>,\n",
       "  <DMatch 0x7f5c2f933c70>,\n",
       "  <DMatch 0x7f5c2f933c90>,\n",
       "  <DMatch 0x7f5c2f933cb0>,\n",
       "  <DMatch 0x7f5c2f933cd0>,\n",
       "  <DMatch 0x7f5c2f933cf0>,\n",
       "  <DMatch 0x7f5c2f933d10>,\n",
       "  <DMatch 0x7f5c2f933d30>,\n",
       "  <DMatch 0x7f5c2f933d50>,\n",
       "  <DMatch 0x7f5c2f933d70>,\n",
       "  <DMatch 0x7f5c2f933d90>,\n",
       "  <DMatch 0x7f5c2f933db0>,\n",
       "  <DMatch 0x7f5c2f933dd0>,\n",
       "  <DMatch 0x7f5c2f933df0>,\n",
       "  <DMatch 0x7f5c2f933e10>,\n",
       "  <DMatch 0x7f5c2f933e30>,\n",
       "  <DMatch 0x7f5c2f933e50>,\n",
       "  <DMatch 0x7f5c2f933e70>,\n",
       "  <DMatch 0x7f5c2f933e90>,\n",
       "  <DMatch 0x7f5c2f933eb0>,\n",
       "  <DMatch 0x7f5c2f933ed0>,\n",
       "  <DMatch 0x7f5c2f933ef0>])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p[1].getMatches(), p[2].getMatches()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_tensorflow2_p36",
   "language": "python",
   "name": "conda_amazonei_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
